{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafab61d",
   "metadata": {},
   "source": [
    "__Общая задача:__ создать скрипт для формирования витрины на основе логов web-сайта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ceac194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd        # импортируем необходимые библиотеки\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "import re\n",
    "import httpagentparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60b82f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# все что нужно для спарка\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import regexp_extract \n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f856d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем спарк-сессию\n",
    "spark = SparkSession.builder \\\n",
    "  .master(\"local[1]\") \\\n",
    "  .appName(\"SparkFirst\") \\\n",
    "  .appName(\"Timeout Troubleshooting\") \\\n",
    "  .config(\"spark.executor.memory\", \"10g\")\\\n",
    "  .config(\"spark.executor.cores\", 5) \\\n",
    "  .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "  .config(\"spark.dynamicAllocation.maxExecutors\", 5) \\\n",
    "  .config(\"spark.network.timeout\", \"600s\") \\\n",
    "  .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a33a21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_df = spark.read.text('access.log')\n",
    "base_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79ffb471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                                                                                                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|54.36.149.41 - - [22/Jan/2019:03:56:14 +0330] \"GET /filter/27|13%20%D9%85%DA%AF%D8%A7%D9%BE%DB%8C%DA%A9%D8%B3%D9%84,27|%DA%A9%D9%85%D8%AA%D8%B1%20%D8%A7%D8%B2%205%20%D9%85%DA%AF%D8%A7%D9%BE%DB%8C%DA%A9%D8%B3%D9%84,p53 HTTP/1.1\" 200 30577 \"-\" \"Mozilla/5.0 (compatible; AhrefsBot/6.1; +http://ahrefs.com/robot/)\" \"-\"|\n",
      "|31.56.96.51 - - [22/Jan/2019:03:56:16 +0330] \"GET /image/60844/productModel/200x200 HTTP/1.1\" 200 5667 \"https://www.zanbil.ir/m/filter/b113\" \"Mozilla/5.0 (Linux; Android 6.0; ALE-L21 Build/HuaweiALE-L21) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.158 Mobile Safari/537.36\" \"-\"                         |\n",
      "|31.56.96.51 - - [22/Jan/2019:03:56:16 +0330] \"GET /image/61474/productModel/200x200 HTTP/1.1\" 200 5379 \"https://www.zanbil.ir/m/filter/b113\" \"Mozilla/5.0 (Linux; Android 6.0; ALE-L21 Build/HuaweiALE-L21) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.158 Mobile Safari/537.36\" \"-\"                         |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_df.show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09be15fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(base_df.filter(base_df['value'].isNull()).count()) # проверим есть ли нулевые строки в исходном датафрейме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36de98da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['54.36.149.41 - - [22/Jan/2019:03:56:14 +0330] \"GET /filter/27|13%20%D9%85%DA%AF%D8%A7%D9%BE%DB%8C%DA%A9%D8%B3%D9%84,27|%DA%A9%D9%85%D8%AA%D8%B1%20%D8%A7%D8%B2%205%20%D9%85%DA%AF%D8%A7%D9%BE%DB%8C%DA%A9%D8%B3%D9%84,p53 HTTP/1.1\" 200 30577 \"-\" \"Mozilla/5.0 (compatible; AhrefsBot/6.1; +http://ahrefs.com/robot/)\" \"-\"',\n",
       " '31.56.96.51 - - [22/Jan/2019:03:56:16 +0330] \"GET /image/60844/productModel/200x200 HTTP/1.1\" 200 5667 \"https://www.zanbil.ir/m/filter/b113\" \"Mozilla/5.0 (Linux; Android 6.0; ALE-L21 Build/HuaweiALE-L21) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.158 Mobile Safari/537.36\" \"-\"',\n",
       " '31.56.96.51 - - [22/Jan/2019:03:56:16 +0330] \"GET /image/61474/productModel/200x200 HTTP/1.1\" 200 5379 \"https://www.zanbil.ir/m/filter/b113\" \"Mozilla/5.0 (Linux; Android 6.0; ALE-L21 Build/HuaweiALE-L21) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.158 Mobile Safari/537.36\" \"-\"',\n",
       " '40.77.167.129 - - [22/Jan/2019:03:56:17 +0330] \"GET /image/14925/productModel/100x100 HTTP/1.1\" 200 1696 \"-\" \"Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\" \"-\"',\n",
       " '91.99.72.15 - - [22/Jan/2019:03:56:17 +0330] \"GET /product/31893/62100/%D8%B3%D8%B4%D9%88%D8%A7%D8%B1-%D8%AE%D8%A7%D9%86%DA%AF%DB%8C-%D9%BE%D8%B1%D9%86%D8%B3%D9%84%DB%8C-%D9%85%D8%AF%D9%84-PR257AT HTTP/1.1\" 200 41483 \"-\" \"Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:16.0)Gecko/16.0 Firefox/16.0\" \"-\"']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_logs = [item['value'] for item in base_df.take(5)]  # выберем небольшую выборку из основного датафрейма для проверки регулярок\n",
    "sample_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6117f8",
   "metadata": {},
   "source": [
    "При помощи регулярных выражений спарсим необходимые данные. Из лога я взяла не все данные, только хост, метод, протокол, статус и самое главное строку с User agent-информацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dfea2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['54.36.149.41', '31.56.96.51', '31.56.96.51', '40.77.167.129', '91.99.72.15']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_pattern = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'              #регулярное выражение для хоста\n",
    "hosts = [re.search(host_pattern, item).group(1)\n",
    "           if re.search(host_pattern, item)\n",
    "           else 'no match'\n",
    "           for item in sample_logs]\n",
    "hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd4f9172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GET',\n",
       "  '/filter/27|13%20%D9%85%DA%AF%D8%A7%D9%BE%DB%8C%DA%A9%D8%B3%D9%84,27|%DA%A9%D9%85%D8%AA%D8%B1%20%D8%A7%D8%B2%205%20%D9%85%DA%AF%D8%A7%D9%BE%DB%8C%DA%A9%D8%B3%D9%84,p53',\n",
       "  'HTTP/1.1'),\n",
       " ('GET', '/image/60844/productModel/200x200', 'HTTP/1.1'),\n",
       " ('GET', '/image/61474/productModel/200x200', 'HTTP/1.1'),\n",
       " ('GET', '/image/14925/productModel/100x100', 'HTTP/1.1'),\n",
       " ('GET',\n",
       "  '/product/31893/62100/%D8%B3%D8%B4%D9%88%D8%A7%D8%B1-%D8%AE%D8%A7%D9%86%DA%AF%DB%8C-%D9%BE%D8%B1%D9%86%D8%B3%D9%84%DB%8C-%D9%85%D8%AF%D9%84-PR257AT',\n",
       "  'HTTP/1.1')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_uri_protocol_pattern = r'\\\"(\\S+) (.*?) (\\S+)\\\"'     #регулярное выражение для метода\n",
    "method_uri_protocol = [re.search(method_uri_protocol_pattern, item).groups()\n",
    "               if re.search(method_uri_protocol_pattern, item)\n",
    "               else 'no match'\n",
    "              for item in sample_logs]\n",
    "method_uri_protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcca7a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['200', '200', '200', '200', '200']\n"
     ]
    }
   ],
   "source": [
    "status_pattern = r'\\s(\\d{3})\\s'                     #регулярное выражение для ответа\n",
    "status = [re.search(status_pattern, item).group(1) for item in sample_logs]\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89fa0f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mozilla/5.0 ',\n",
       " 'Mozilla/5.0 ',\n",
       " 'Mozilla/5.0 ',\n",
       " 'Mozilla/5.0 ',\n",
       " 'Mozilla/5.0 ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_agent_top=r'\\S+\\S+\\\".\\\"([^\\(]+)\\((.*)\\)'      #регулярное выражение для начала UA-строки\n",
    "user_agent = [re.search(user_agent_top, item).group(1) for item in sample_logs]\n",
    "user_agent                              #мы его использовать в итоговой таблице не будем так, как оно для всех одинаковое и не несет полезной информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b386da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['compatible; AhrefsBot/6.1; +http://ahrefs.com/robot/)\"',\n",
       " 'Linux; Android 6.0; ALE-L21 Build/HuaweiALE-L21) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.158 Mobile Safari/537.36\"',\n",
       " 'Linux; Android 6.0; ALE-L21 Build/HuaweiALE-L21) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.158 Mobile Safari/537.36\"',\n",
       " 'compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\"',\n",
       " 'Windows NT 6.2; Win64; x64; rv:16.0)Gecko/16.0 Firefox/16.0\"']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_agent_end=r'\\((.*) '                       #регулярнео выражение для окончания UA-строки\n",
    "user_agent = [re.search(user_agent_end, item).group(1) for item in sample_logs]\n",
    "user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6491902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+--------------------+\n",
      "|         host|method|status|      user_agent_end|\n",
      "+-------------+------+------+--------------------+\n",
      "| 54.36.149.41|   GET|   200|compatible; Ahref...|\n",
      "|  31.56.96.51|   GET|   200|Linux; Android 6....|\n",
      "|  31.56.96.51|   GET|   200|Linux; Android 6....|\n",
      "|40.77.167.129|   GET|   200|compatible; bingb...|\n",
      "|  91.99.72.15|   GET|   200|Windows NT 6.2; W...|\n",
      "|40.77.167.129|   GET|   200|compatible; bingb...|\n",
      "|40.77.167.129|   GET|   200|compatible; bingb...|\n",
      "|40.77.167.129|   GET|   200|compatible; bingb...|\n",
      "|66.249.66.194|   GET|   200|compatible; Googl...|\n",
      "|40.77.167.129|   GET|   200|compatible; bingb...|\n",
      "+-------------+------+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "(10365152, 4)\n"
     ]
    }
   ],
   "source": [
    "logs_df = base_df.select(regexp_extract('value', host_pattern, 1).alias('host'),           # теперь объединяем все вместе\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 1).alias('method'),\n",
    "                         regexp_extract('value', status_pattern, 1).cast('integer').alias('status'),\n",
    "                         regexp_extract('value', user_agent_end, 1).alias('user_agent_end'))\n",
    "logs_df.show(10, truncate=True)\n",
    "print((logs_df.count(), len(logs_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a9832",
   "metadata": {},
   "source": [
    "Проверим схему нашего датафрейма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f2ee7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- user_agent_end: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c18e694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_rows_df = logs_df.filter(logs_df['host'].isNull()| \n",
    "                             logs_df['method'].isNull() |\n",
    "                             logs_df['status'].isNull() |\n",
    "                             logs_df['user_agent_end'].isNull())                    \n",
    "bad_rows_df.count()   #пустых строчек нет, это хорошо!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854334a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbb366f3",
   "metadata": {},
   "source": [
    "Приступим к проведению анализа и построению витрины:\n",
    "- Суррогатный ключ устройства\n",
    "\n",
    "- Название устройства\n",
    "\n",
    "- Количество пользователей\n",
    "\n",
    "- Доля пользователей данного устройства от общего числа пользователей\n",
    "\n",
    "- Количество совершенных действий для данного устройства\n",
    "\n",
    "- Доля совершенных действий с данного устройства относительно других устройств\n",
    "\n",
    "- Список из 5 самых популярных браузеров, используемых на данном устройстве различными пользователями, с указанием доли использования для данного браузера относительно остальных браузеров \n",
    "\n",
    "- Количество ответов сервера, отличных от 200 на данном устройстве\n",
    "\n",
    "- Для каждого из ответов сервера, отличных от 200, сформировать поле, в котором будет содержаться количество ответов данного типа"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235fadcc",
   "metadata": {},
   "source": [
    "Для определения устройства и браузера воспользуемся готовым парсером (https://pypi.org/project/httpagentparser/#description) (импортировали его вначале):\n",
    "\n",
    "Метод httpagentparser.detect(s) дает ответ вида (пример) {'platform': {'name': 'Android', 'version': '6.0'}, 'os': {'name': 'Linux'}, 'bot': False, 'dist': {'name': 'Android', 'version': '6.0'}, 'browser': {'name': 'Chrome', 'version': '66.0.3359.158'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7cf1f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Платформа Android ,браузер Chrome\n"
     ]
    }
   ],
   "source": [
    "#тестовая строка\n",
    "s='Linux; Android 6.0; ALE-L21 Build/HuaweiALE-L21) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.158 Mobile Safari/537.36'\n",
    "y=httpagentparser.detect(s)\n",
    "print('Платформа',y['platform']['name'],',браузер', y['browser']['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6aedd087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@F.udf                 # создадим пользовательскую функцию на основе парсера, которая возвращает название платформы\n",
    "def device_def(x):\n",
    "    y= httpagentparser.detect(x)\n",
    "    if 'platform' in y:\n",
    "        return y['platform']['name'] \n",
    "    else:\n",
    "        return 'None'      # ниже протестируем работу функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "08684770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Android'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_def('Linux; Android 6.0; ALE-L21 Build/HuaweiALE-L21) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.158 Mobile Safari/537.36')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8e977cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_def('compatible; AhrefsBot/6.1; +http://ahrefs.com/robot/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4aa0db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------+--------------------+--------+\n",
      "|        host|method|status|      user_agent_end|platform|\n",
      "+------------+------+------+--------------------+--------+\n",
      "|54.36.149.41|   GET|   200|compatible; Ahref...|    null|\n",
      "| 31.56.96.51|   GET|   200|Linux; Android 6....|    null|\n",
      "| 31.56.96.51|   GET|   200|Linux; Android 6....|    null|\n",
      "+------------+------+------+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_platform = logs_df.withColumn('platform', lit(device_def(col('user_agent_end'))))\\\n",
    "                .show(3)                                  # создадим новый столбец с платформой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "837c17a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf                 # создадим пользовательскую функцию на основе парсера, которая возвращает название браузера\n",
    "def browser_def(x:str)->str:\n",
    "    y= httpagentparser.detect(x)\n",
    "    if 'browser' in y:\n",
    "        return f\"{y['browser']['name']}\" \n",
    "    else:\n",
    "        return 'None'                   # ниже протестируем работу функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2089fc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chrome'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "browser_def('Linux; Android 6.0; ALE-L21 Build/HuaweiALE-L21) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.158 Mobile Safari/537.36')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "af74b250",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o553.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 1 times, most recent failure: Lost task 0.0 in stage 81.0 (TID 427) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m logs_browser\u001b[38;5;241m=\u001b[39mlogs_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowser\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(browser_def(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_agent_end\u001b[39m\u001b[38;5;124m\"\u001b[39m))))\\\n\u001b[1;32m----> 2\u001b[0m          \u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical))\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o553.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 1 times, most recent failure: Lost task 0.0 in stage 81.0 (TID 427) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "logs_browser=logs_df.withColumn(\"browser\", lit(browser_def(F.col(\"user_agent_end\"))))\\\n",
    "         .show(3)                                  # применим функцию и создадим новый столбец с браузером "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ee182",
   "metadata": {},
   "source": [
    "Кэшируем logs_df для дальнейшего анализа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51109c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3adc3",
   "metadata": {},
   "source": [
    "Количество пользователей - это количество уникальных хостов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d1a5bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT host)|\n",
      "+--------------------+\n",
      "|              258606|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_cust = logs_df.select(count_distinct(\"host\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ac788",
   "metadata": {},
   "source": [
    "Количество пользователей устройства - это пользователи, сгруппированные по устройству:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device_users=logs_df.select('host').groupby('platform').count().sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d90819",
   "metadata": {},
   "source": [
    "Количество совершенных действий для данного устройства - это группировка по колонке и подсчет методов (GET, POST):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24962528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device_actions=logs_df.select('method').groupby('platform').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c8354",
   "metadata": {},
   "source": [
    "Всего действий - это подсчет значений колонки методов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74478122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10365152"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_actions = logs_df.select('method').count()\n",
    "total_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c4babe",
   "metadata": {},
   "source": [
    "Доля совершенных действий с данного устройства относительно других устройств - это "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9265e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "349348db",
   "metadata": {},
   "source": [
    "Список из 5 самых популярных браузеров, используемых на данном устройстве различными пользователями, с указанием доли использования для данного браузера относительно остальных браузеров - это "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_b= logs_df.select('browser').groupby('platform').count().sort(\"count\", ascending=False).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d434fd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|status|  count|\n",
      "+------+-------+\n",
      "|   200|9579825|\n",
      "|   304| 340228|\n",
      "|   302| 199835|\n",
      "|   404| 105011|\n",
      "|   301|  67552|\n",
      "|   499|  50852|\n",
      "|   500|  14266|\n",
      "|   403|   5634|\n",
      "|   502|    798|\n",
      "|   400|    376|\n",
      "|   401|    323|\n",
      "|   777|    211|\n",
      "|   408|    112|\n",
      "|   504|    103|\n",
      "|   414|     17|\n",
      "|   405|      6|\n",
      "|   206|      3|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_status=logs_df.groupBy('status').count().sort('count', ascending=False).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d89d71",
   "metadata": {},
   "source": [
    "Количество ответов сервера, отличных от 200 на данном устройстве:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aa249103",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `platform` cannot be resolved. Did you mean one of the following? [`method`, `status`, `host`, `user_agent_end`].;\n'Aggregate ['platform], ['platform, count(1) AS count#716L]\n+- Filter NOT (status#155 = cast(200 as int))\n   +- Project [regexp_extract(value#0, (^\\S+\\.[\\S+\\.]+\\S+)\\s, 1) AS host#153, regexp_extract(value#0, \\\"(\\S+) (.*?) (\\S+)\\\", 1) AS method#154, cast(regexp_extract(value#0, \\s(\\d{3})\\s, 1) as int) AS status#155, regexp_extract(value#0, \\((.*) , 1) AS user_agent_end#156]\n      +- Relation [value#0] text\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m answers_ne200 \u001b[38;5;241m=\u001b[39m logs_df\u001b[38;5;241m.\u001b[39mfilter(logs_df\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplatform\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m      2\u001b[0m answers_ne200\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\pyspark\\sql\\group.py:38\u001b[0m, in \u001b[0;36mdfapi.<locals>._api\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_api\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroupedData\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m     37\u001b[0m     name \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m---> 38\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jgd, name)()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PySpark\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `platform` cannot be resolved. Did you mean one of the following? [`method`, `status`, `host`, `user_agent_end`].;\n'Aggregate ['platform], ['platform, count(1) AS count#716L]\n+- Filter NOT (status#155 = cast(200 as int))\n   +- Project [regexp_extract(value#0, (^\\S+\\.[\\S+\\.]+\\S+)\\s, 1) AS host#153, regexp_extract(value#0, \\\"(\\S+) (.*?) (\\S+)\\\", 1) AS method#154, cast(regexp_extract(value#0, \\s(\\d{3})\\s, 1) as int) AS status#155, regexp_extract(value#0, \\((.*) , 1) AS user_agent_end#156]\n      +- Relation [value#0] text\n"
     ]
    }
   ],
   "source": [
    "answers_ne200 = logs_df.filter(logs_df.status != '200').groupby('platform').count()\n",
    "answers_ne200.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4945c",
   "metadata": {},
   "source": [
    "Для каждого из ответов сервера, отличных от 200, сформировать поле, в котором будет содержаться количество ответов данного типа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d168d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0780c564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a0322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda010cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_con = psycopg2.connect(database='testdb',     # создадим подключение к созданной базе данных exrate\n",
    "                        user='postgres',\n",
    "                        password='pass',\n",
    "                        host='localhost',\n",
    "                        port=5435)\n",
    "cur = db_con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ebcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\" CREATE TABLE btc_Jan2023(\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    date DATE,\n",
    "    val_id VARCHAR,\n",
    "    base_rate DECIMAL,\n",
    "    val_base VARCHAR\n",
    ")\n",
    "\"\"\") \n",
    "db_con.commit()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install httpagentparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpagentparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea331e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e48835",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/532.9 (KHTML, like Gecko) \\\n",
    "        Chrome/5.0.307.11 Safari/532.9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18728d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(httpagentparser.simple_detect(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c8d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd0b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "db_con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
